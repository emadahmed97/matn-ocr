# Arabic OCR for Classical Islamic Texts - Updated Implementation Plan

*Based on existing `notebooks/arabic_ocr_finetune.ipynb` with DeepSeek-OCR + Unsloth*

## Project Overview

We have a working Arabic OCR system using:
- **Model**: DeepSeek-OCR (fine-tuned with Unsloth)
- **Dataset**: `mssqpi/Arabic-OCR-Dataset`
- **Method**: LoRA fine-tuning with custom data collator
- **Performance**: 74% CER improvement (23% â†’ 6% on sample)

## Current State Analysis

### âœ… Already Completed (from notebook):
1. **Model Setup**: DeepSeek-OCR with Unsloth integration
2. **Dataset**: Arabic OCR dataset (2,160,000 samples)
3. **Data Processing**: Custom `DeepSeekOCRDataCollator`
4. **Training Pipeline**: LoRA fine-tuning (77M/3.4B params trained)
5. **Inference**: Working OCR with dynamic image preprocessing
6. **Model Saving**: Both LoRA adapters and merged 16-bit model

### ðŸ”² Missing for ML School Integration:
1. **Metaflow Pipeline Structure**: Convert notebook â†’ production pipelines
2. **MLflow Integration**: Experiment tracking and model registry
3. **Cross-validation**: Adapt for OCR evaluation metrics
4. **Monitoring Pipeline**: Model quality and data drift detection
5. **Deployment Pipeline**: API serving and scaling
6. **Testing Framework**: Automated evaluation and CI/CD

## Updated Implementation Plan

### Phase 1: Pipeline Extraction & Adaptation
*Convert working notebook code to ML School structure*

#### 1.1 Extract Core Components âœ…
- âœ… DeepSeek-OCR model loading
- âœ… Custom data collator
- âœ… Training configuration
- âœ… Inference pipeline
- ðŸ”² Create reusable modules from notebook cells

#### 1.2 Create Training Pipeline
- ðŸ”² `pipelines/arabic_training.py` - Replace penguin training
- ðŸ”² Integrate `DeepSeekOCRDataCollator`
- ðŸ”² Add MLflow experiment tracking
- ðŸ”² Implement OCR evaluation metrics (CER, WER, BLEU)
- ðŸ”² Cross-validation for Arabic OCR

#### 1.3 Dataset Integration
- ðŸ”² Replace penguins dataset with `mssqpi/Arabic-OCR-Dataset`
- ðŸ”² Add data preprocessing utilities
- ðŸ”² Implement image-text pair handling
- ðŸ”² Create data quality validation

### Phase 2: ML School Pipeline Adaptation

#### 2.1 Training Pipeline (`pipelines/training.py`)
```python
# Key adaptations needed:
- DatasetMixin â†’ ArabicOCRDatasetMixin
- build_model() â†’ load_deepseek_ocr_model()
- Classification metrics â†’ OCR metrics (CER/WER)
- Cross-validation â†’ Text-based splitting
- MLflow logging â†’ OCR-specific artifacts
```

#### 2.2 Inference Pipeline (`pipelines/inference/`)
```python
# Integrate from notebook:
- model.infer() method
- Dynamic image preprocessing
- Custom PyFunc wrapper for MLflow
- Arabic text post-processing
```

#### 2.3 Monitoring Pipeline (`pipelines/monitoring.py`)
```python
# New components:
- OCR accuracy drift detection
- Character/word error rate tracking
- Arabic text quality validation
- Image quality assessment
```

### Phase 3: Production Integration

#### 3.1 Model Registry & Versioning
- ðŸ”² Register fine-tuned DeepSeek-OCR models
- ðŸ”² Version LoRA adapters and merged models
- ðŸ”² Model metadata and performance tracking
- ðŸ”² A/B testing framework for OCR models

#### 3.2 Serving & Deployment
- ðŸ”² MLflow serving integration
- ðŸ”² REST API for OCR endpoints
- ðŸ”² Batch processing capabilities
- ðŸ”² Performance optimization (GPU/CPU)

#### 3.3 AWS Deployment
- ðŸ”² SageMaker endpoint for DeepSeek-OCR
- ðŸ”² S3 storage for manuscript images
- ðŸ”² Lambda functions for preprocessing
- ðŸ”² CloudFormation templates

## Key Technical Components to Extract

### From Notebook Cell #3: Model Loading
```python
from unsloth import FastVisionModel
from transformers import AutoModel

model, tokenizer = FastVisionModel.from_pretrained(
    "./deepseek_ocr",
    load_in_4bit=False,
    auto_model=AutoModel,
    trust_remote_code=True,
    use_gradient_checkpointing="unsloth"
)
```

### From Notebook Cell #22: Data Collator
```python
class DeepSeekOCRDataCollator:
    # Full implementation for image-text processing
    # Dynamic preprocessing with crop modes
    # Attention mask and label creation
```

### From Notebook Cell #24: Training Configuration
```python
trainer = Trainer(
    model=model,
    data_collator=DeepSeekOCRDataCollator(...),
    args=TrainingArguments(
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,
        learning_rate=2e-4,
        # OCR-specific training params
    )
)
```

## Immediate Next Steps

### Step 1: Extract and Modularize
1. **Create `pipelines/arabic_ocr/`** - New module structure
2. **Extract model loading** â†’ `arabic_ocr/model.py`
3. **Extract data collator** â†’ `arabic_ocr/data_collator.py`
4. **Extract preprocessing** â†’ `arabic_ocr/preprocessing.py`

### Step 2: Adapt Existing Pipelines
1. **Modify `pipelines/training.py`**:
   - Replace DatasetMixin with ArabicOCRDatasetMixin
   - Change evaluation metrics to CER/WER
   - Integrate DeepSeek-OCR model loading

2. **Update `pipelines/inference/`**:
   - Replace classification with OCR inference
   - Add image preprocessing pipeline
   - Implement text post-processing

### Step 3: Testing & Validation
1. **Create test suite** based on notebook results
2. **Benchmark against notebook performance** (6% CER target)
3. **Validate MLflow integration**
4. **Test cross-validation strategy**

## Success Criteria

### Performance Targets (from notebook):
- âœ… **Character Error Rate < 6%** (already achieved)
- âœ… **Training Efficiency**: 1.6GB memory for LoRA training
- âœ… **74% CER improvement** over baseline
- ðŸ”² **End-to-end pipeline** < 2 minutes training time
- ðŸ”² **Inference speed** < 2 seconds per image

### Integration Targets:
- ðŸ”² **Seamless MLflow tracking** for OCR experiments
- ðŸ”² **Production deployment** with auto-scaling
- ðŸ”² **Monitoring dashboard** for Arabic OCR quality
- ðŸ”² **CI/CD pipeline** with automated testing

## File Structure Changes

```
pipelines/
â”œâ”€â”€ arabic_ocr/                    # New: Extracted from notebook
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ model.py                   # DeepSeek-OCR loading
â”‚   â”œâ”€â”€ data_collator.py          # Custom OCR data collator
â”‚   â”œâ”€â”€ preprocessing.py          # Image processing utilities
â”‚   â”œâ”€â”€ metrics.py                # CER, WER, BLEU evaluation
â”‚   â””â”€â”€ inference.py              # OCR inference pipeline
â”œâ”€â”€ training.py                    # Modified: Use Arabic OCR
â”œâ”€â”€ inference/
â”‚   â”œâ”€â”€ model.py                   # Modified: OCR PyFunc model
â”‚   â””â”€â”€ backend.py                 # Modified: Arabic text serving
â””â”€â”€ monitoring.py                  # Modified: OCR quality monitoring
```

---

*This plan leverages the proven notebook implementation while adapting it to the ML School production framework.*